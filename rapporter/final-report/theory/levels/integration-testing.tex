Since a unit test only assures that a single unit works as expected,
faults may still reside in how the units works together. The purpose of
integration testing is to test several individual units together, in
order to see if they still work together as intended.\\

There are several ways of performing integration testing, as well as
arguments and opinions about the different ways. \citet{book:adp} state
that integration tests should be built incrementally by extending unit
tests. The unit tests are extended and merged so that they span over
multiple units. The scope of the tests is increased gradually, and both
valid and invalid data is given into the integration tested system unit
in order to test the interfaces between smaller units. Since this
process is done gradually, it is possible to see which parts of the
integrated unit that is faulty by examining which parts have been
integrated since the latest test run.\\

\citet{book:pfleeger} refer to the type of integration testing described
by \citeauthor{book:adp} as \emph{bottom-up testing}, since several
low-level modules (modules at the bottom level of the software) are
integrated into higher-level modules. Multiple other integration
approaches such as \emph{top-down testing} and \emph{sandwich testing}
are also mentioned, and the difference between the approaches is in which
order the units are integrated.\\

\citet{video:integrated_scam} criticizes one kind of integration tests,
which he refers to as \emph{integrated tests}. This refers to
integration tests that are testing the functionality of multiple units
in the same way as unit tests, i.e. by input data and examine the
output. When testing multiple units in this way, one loses the ability
to see which part of all the tested units that are actually failing. As
the number of tested units rises, the number of possible paths will grow
exponentially. This makes it hard to see the reason for a failed test,
but also makes it very hard to decide which of all this paths that needs
to be tested. \citeauthor{video:integrated_scam} claims that this fact
makes developers sloppier, which increases the risk of introducing
mistakes that goes unnoticed through the test suite. If the problem is
solved by writing even more integration tests, developers have less time
to do proper unit tests and instead introduce more sloppy designs. One
may however argue that this argument is based on the fact that
integrated tests to a large part are used instead of unit tests, rather
than as a complement as suggested by \citeauthor{book:pfleeger} and
\citeauthor{book:adp}.\\

Instead of integrated tests, another type of integration tests called
contract- and collaboration tests are proposed. The purpose of these
tests is to verify the interface between all unit-tested modules by
using mocks to test that Unit A tries to invoke the expected methods on
Unit B (contract test). In order to avoid errors due to mocking, tests
are also needed to make sure that Unit B really responds to the calls
that are expected to be performed by Unit A in the contract test. The
idea of this is to build a chain of trust inside our own software via
transitivity. This means that if Unit A and Unit B works together as
expected, and Unit B and Unit C also works together as expected, Unit A
and Unit C will also work together as expected.\\
